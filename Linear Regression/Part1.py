# -*- coding: utf-8 -*-
"""Assignment1_part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ucTWgC4sjZV9zx48V1WzYP_JNPc5Zafs

#Assignment 1: Part 1 - Linear Regression Using Gradient Descent
#Submitted by:
##1. Yagna Srinivasa Harsha Annadata(yxa210024)
##2. Vishruth Reddy Chinthi Reddy(vxc220020)
- Train a linear model on a dataset
  - Implement the model from scratch (linearRegression class)
  - Pre-Processing of the dataset (Dataset used - https://archive.ics.uci.edu/dataset/186/wine+quality)
  - Use learning curve and other plots to understand more about the linear model
  - Tune parameters such as learning rate, no of iterations etc. to achieve the optimum error value
  - Measure the model using various evaluation statistics - MSE, MAE, R2.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
import seaborn as sns
import matplotlib.pyplot as plt
import requests
import csv
from io import StringIO

"""Define Linear Regression class"""

class linearRegression:
    def custom_init(self):
        pass

    def predict(self, X, theta):
        return X @ theta

    def cost(self,theta, X, y):
        m = len(y)
        error = X @ theta - y
        cost_val = (1 / (2 * m)) * np.transpose(error) @ error
        return cost_val

    def gradientDescent(self, X, y, theta, learning_rate, iterations):
      m = len(y)
      cost_history = []

      for i in range(iterations):
        # Compute predictions and errors
        predictions = X @ theta
        errors = predictions - y
        theta = theta - (learning_rate / m) * (X.T @ errors)

        # Compute and store cost
        cost_val = np.mean(errors**2)
        cost_history.append(cost_val)

      return theta,cost_history

"""Load the dataset and rename the columns

github link: https://raw.githubusercontent.com/YagnaAnnadata/Projects/main/LinearRegression/Dataset/winequality/winequality-red.csv

"""

path="https://raw.githubusercontent.com/YagnaAnnadata/Projects/main/LinearRegression/Dataset/winequality/winequality-red.csv"

response = requests.get(path)
csv_data = StringIO(response.text)
df_train = pd.read_csv(csv_data, delimiter=';')
print("########## NULL Values in dataset ###########")
print(df_train.isna().sum())
df_train.corr()

df_train

"""Function to normlize"""

df = df_train.copy()

def normal(x):
        if len(x) == 0:
            return x
        x_mean = np.mean(x)
        x_max = max(x)
        x_min = min(x)
        return (x - x_mean) / (x_max - x_min)

df = df.apply(normal, axis=0)

"""Correlation plot"""

correlation = df_train.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(correlation, dtype=bool)
mask[np.triu_indices_from(mask)] = True

fig, ax = plt.subplots(figsize=(10, 8))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=100)

# Plot the heatmap with the mask and aspect ratio
sns.heatmap(correlation, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5)
fig.suptitle('Correlation matrix of features', fontsize=15)
ax.text(0.77, 0.2, '', fontsize=13, ha='center', va='center',
         transform=ax.transAxes, color='grey', alpha=0.5)

fig.tight_layout()

sns.pairplot(df)

"""Scatter plot of some variables with target"""

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Scatterplot 1: Citric Acid vs Quality
sns.scatterplot(x=df_train['citric acid'], y=df_train['quality'], ax=axes[0])
axes[0].set_xlabel('Citric Acid')
axes[0].set_ylabel('Quality')
axes[0].set_title('Citric Acid vs Quality')

# Scatterplot 2: pH vs Quality
sns.scatterplot(x=df_train['pH'], y=df_train['quality'], ax=axes[1])
axes[1].set_xlabel('pH')
axes[1].set_ylabel('Quality')
axes[1].set_title('pH vs Quality')

# Scatterplot 3: Sulphates vs Quality
sns.scatterplot(x=df_train['sulphates'], y=df_train['quality'], ax=axes[2])
axes[2].set_xlabel('Sulphates')
axes[2].set_ylabel('Quality')
axes[2].set_title('Sulphates vs Quality')

plt.tight_layout()

# Show the plots
plt.show()

"""Train Test Split"""

linear = linearRegression()
# Split the dataset into train and test sets
train, test = train_test_split(df, test_size=0.1, random_state=40)
# Convert data to NumPy arrays
X_train = train.drop(columns=['quality']).to_numpy()
X_test = test.drop(columns=['quality']).to_numpy()
y_train = train['quality'].to_numpy()
y_test = test['quality'].to_numpy()
# Initialize theta to ones
theta = np.ones(X_train.shape[1])*X_test.mean()

"""Gradient Descent"""

learning_rate = 0.01
iterations = 100000

theta,cost_history = linear.gradientDescent(X_train,y_train,theta,learning_rate, iterations)
# Creating a plot
plt.figure(figsize=(8,6))
plt.plot(np.arange(learning_rate, iterations),cost_history, color = 'green')
plt.xlabel('Iteration');
plt.ylabel('Cost Function(MSE)');
plt.title('Vectorization: MSE Trend over Iterations')

"""Plot of predicted vs observed values of Target"""

Y_max = y_test.max()
Y_min = y_test.min()

axes = sns.scatterplot(x=linear.predict(X_test,theta), y=y_test)
axes.set(ylim=(Y_min, Y_max))
axes.set(xlim=(Y_min, Y_max))
axes.set_xlabel("Predicted value of quality")
axes.set_ylabel("Observed value of quality")

X_ref = Y_ref = np.linspace(Y_min, Y_max, 100)
plt.plot(X_ref, Y_ref, color='green', linewidth=1)
plt.show()

"""Evaluate Performance"""

y_train_predict = linear.predict(X_train, theta)
y_test_predict = linear.predict(X_test, theta)

mae_train = mean_absolute_error(y_train,y_train_predict)
mse_train = mean_squared_error(y_train,y_train_predict)
r2_train = r2_score(y_train, y_train_predict)

mae_test = mean_absolute_error(y_test,y_test_predict)
mse_test = mean_squared_error(y_test,y_test_predict)
r2_test = r2_score(y_test,y_test_predict)

df_metrics = pd.DataFrame([[mae_train, mae_test], [mse_train, mse_test], [r2_train, r2_test]],
             index = ["Mean Absolute Error", "Mean Square Error", "R2 Score"],
             columns = ["train", "test"])

display(df_metrics)

print()
print(" ------ Training Performance ------ ")
print("- Mean Squared Error =",mae_train)
print("- Mean Absolute Error =",mse_train)
print("- R2 Score =",r2_train)
print()
print(" ------ Testing Performance ------ ")
print("- Mean Squared Error =",mae_test)
print("- Mean Absolute Error =",mse_test)
print("- R2 Score =",r2_test)